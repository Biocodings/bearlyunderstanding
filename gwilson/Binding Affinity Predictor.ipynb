{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()\n",
    "    \n",
    "\n",
    "DATA      = r'C:\\Users\\Ekpyrotic\\Documents\\Tensorflow stuff\\binding affinity predictor\\one_hot_encoded_dat.csv'\n",
    "HLA_distances  = r'C:\\Users\\Ekpyrotic\\Documents\\Tensorflow stuff\\binding affinity predictor\\crystal structures\\HLAdistmatrix.csv'\n",
    "HLA_dist = np.genfromtxt(HLA_distances, delimiter=',')\n",
    "#1: HLA-A*:0201\n",
    "#2: HLA-B*58:01\n",
    "#3: HLA-B*57:01\n",
    "\n",
    "newrows   = []\n",
    "with open(DATA) as csvfile:\n",
    "    i = 0\n",
    "    filereader = csv.reader(csvfile)\n",
    "    for row in filereader:\n",
    "        newrow = row\n",
    "        if row[0] == 'HLA-A*02:01':\n",
    "            i = i + 1\n",
    "            newrow[181] = HLA_dist[0,0]\n",
    "        elif row[0] == 'HLA-B*57:01':\n",
    "            newrow[181] = HLA_dist[1,0]\n",
    "        elif row[0] == 'HLA-B*57:01':\n",
    "            newrow[181] = HLA_dist[2,0]\n",
    "        else:\n",
    "            newrow = []         #get rid of other HLA types\n",
    "            #newrow[181] = 999   #keep other HLA type rows\n",
    "        if newrow:\n",
    "            newrows.append(newrow)\n",
    "\n",
    "j = int(np.floor(i/5))\n",
    "\n",
    "#column 0: HLA type\n",
    "#column 1-181: one hot encoding of aa values\n",
    "#column 182: HLA PH distance calc\n",
    "#column 183: observed binding affinity\n",
    "\n",
    "arrayform = np.asarray(newrows)\n",
    "\n",
    "train     = arrayform[j:,1:].astype(np.float)\n",
    "np.random.shuffle(train)\n",
    "train_x   =  train[:, 0:181]\n",
    "train_y   =  train[:,-1]\n",
    "\n",
    "test      = arrayform[0:j,1:].astype(np.float)\n",
    "test_set_data   =  test[:, 0:181]\n",
    "test_set_target =  test[:,-1]\n",
    "\n",
    "\n",
    "folds = KFold(train_x.shape[0], n_folds= 5)\n",
    "foldvals = list(folds)\n",
    "\n",
    "\n",
    "def get_train_inputs(i):\n",
    "    train, test = foldvals[i]\n",
    "    x = tf.constant(train_x[train,:])    \n",
    "    y = tf.constant(train_y[train])\n",
    "    return x, y\n",
    "\n",
    "def get_val_inputs(i):\n",
    "    train, test = foldvals[i]\n",
    "    x = tf.constant(train_x[test,:])\n",
    "    y = tf.constant(train_y[test])\n",
    "    return x, y\n",
    "\n",
    "def get_test_inputs():\n",
    "    x = tf.constant(test_set_data)          \n",
    "    y = tf.constant(test_set_target)\n",
    "    return x, y\n",
    "\n",
    "#now normalize our inputs and predictors \n",
    "def maxminNormalize(array):\n",
    "    normalized = np.divide(np.subtract(array, np.amin(array)), \n",
    "                           np.subtract(np.amax(array), np.amin(array)))\n",
    "    return normalized\n",
    "\n",
    "train_x = maxminNormalize(train_x) #topology\n",
    "#train_x = maxminNormalize(train_x[:,:-1]) #no topology\n",
    "train_y = maxminNormalize(train_y)\n",
    "\n",
    "test_set_data   = maxminNormalize(test_set_data)  #topology\n",
    "#test_set_data   = maxminNormalize(test_set_data[:,:-1])  #no topology\n",
    "test_set_target = maxminNormalize(test_set_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18843"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Test MSE: 0.000229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "features = [tf.contrib.layers.real_valued_column(\"\",dimension = 181)]       #9mer data + distance from HLA a0201 \n",
    "#features = [tf.contrib.layers.real_valued_column(\"\",dimension = 180)]      #9mer data only\n",
    "\n",
    "accuracy_score = np.zeros(5)  \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Monitor performance on a validation set --> early stopping if we don't improve performance after 200 steps\n",
    "validation_metrics = {\n",
    "    \"accuracy\":\n",
    "        tf.contrib.learn.MetricSpec(\n",
    "            metric_fn=tf.contrib.metrics.streaming_accuracy,\n",
    "            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),\n",
    "    \"precision\":\n",
    "        tf.contrib.learn.MetricSpec(\n",
    "            metric_fn=tf.contrib.metrics.streaming_precision,\n",
    "            prediction_key=tf.contrib.learn.PredictionKey.CLASSES),\n",
    "    \"recall\":\n",
    "        tf.contrib.learn.MetricSpec(\n",
    "            metric_fn=tf.contrib.metrics.streaming_recall,\n",
    "            prediction_key=tf.contrib.learn.PredictionKey.CLASSES)\n",
    "}\n",
    "\n",
    "    \n",
    "# train our networks\n",
    "for i in range(0,5):\n",
    "    \n",
    "    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "                    input_fn = lambda:get_val_inputs(i),\n",
    "                    eval_steps=1,  # fix an issue\n",
    "                    every_n_steps=50,\n",
    "                    #metrics=validation_metrics,\n",
    "                    early_stopping_metric='loss',\n",
    "                    early_stopping_metric_minimize=True,\n",
    "                    early_stopping_rounds=200)\n",
    "    \n",
    "    classifier = tf.contrib.learn.DNNRegressor(hidden_units = [55,20,10],feature_columns=features,\n",
    "                                            optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                            learning_rate=0.01,\n",
    "                                            ),\n",
    "                                            #optimizer = tf.train.AdadeltaOptimizer(learning_rate = alpha_vals),\n",
    "                                            config = tf.contrib.learn.RunConfig(save_checkpoints_steps = 50, save_checkpoints_secs = None)\n",
    "                                            )\n",
    "    \n",
    "    classifier.fit(input_fn = lambda:get_train_inputs(i), steps = 20000, monitors = [validation_monitor])\n",
    "    \n",
    "    #accuracy_score[i] = classifier.evaluate(input_fn=get_test_inputs, steps = 500)['loss']\n",
    "    accuracy_score[i] = classifier.evaluate(input_fn=lambda:get_val_inputs(i), steps = 500)['loss']\n",
    "    #accuracy_score[i] = classifier.evaluate(x = test_set_data, y = test_set_target, steps = 500)['loss']\n",
    "    print(i)\n",
    "\n",
    "print(\"\\nTest MSE: {0:f}\\n\".format(np.average(accuracy_score)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.82698487758e-05\n"
     ]
    }
   ],
   "source": [
    "accuracy_score[i] = classifier.evaluate(input_fn=lambda:get_val_inputs(i), steps = 500)['loss']\n",
    "print(accuracy_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global_step': 20, 'loss': 3.4737761e+09}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(input_fn=get_test_inputs,steps = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0011428 ,  0.00114273,  0.00114245,  0.00114324,  0.00114224,\n",
       "        0.0011427 ,  0.00114226,  0.00114199,  0.00114431,  0.0011426 ])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score (no top)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
